Type,Prompt
Unacceptable risk,"Does the AI deliver subliminal or other below‑awareness stimuli intended to influence user behaviour or choices?"
Unacceptable risk,"Is the system intentionally designed to manipulate or deceive users in high‑stakes domains (health, finance, legal, employment) so that their behaviour is materially distorted?"
Unacceptable risk,"Does the AI exploit a user’s vulnerability (age, disability, or severe social/economic hardship) in a way that is likely to make them act against their own interests in a consequential situation?"
Unacceptable risk,"Does the AI generate or update a composite “social score” that can trigger negative or disproportionate treatment in areas unrelated to the behaviour assessed?"
Unacceptable risk,"Is the system used to predict criminal propensity or policing decisions based solely on profiling factors, without direct evidence of wrongdoing?"
Unacceptable risk,"Does the provider build or expand biometric databases (e.g., faces) by scraping public images or CCTV footage without targeted consent or explicit legal basis?"
Unacceptable risk,"Is the AI a live (real‑time) remote biometric identification system deployed in public spaces by law enforcement without a specific judicial or administrative warrant?"
Unacceptable risk,"Does the AI infer emotions of employees or students in real time for monitoring or evaluation purposes without their explicit, informed consent?"
Unacceptable risk,"Does the AI process biometric data to deduce sensitive attributes (race, religion, political views, sexual orientation) without explicit consent?"

High risk,"Is the AI a remote biometric identification tool (non‑real‑time) that recognises individuals at a distance for authentication or surveillance?"
High risk,"Does the AI detect or classify emotional states from faces, voices, or physiological signals to inform automated decisions?"
High risk,"Is the AI a safety‑critical component governing essential infrastructure such as road‑traffic control, energy, or data‑centre operations?"
High risk,"Does the system decide admission, progression, or exam integrity within education or vocational training?"
High risk,"Is the AI used for hiring, promotion, task allocation, termination, or continuous employee monitoring?"
High risk,"Does the system determine eligibility, amount, or revocation of public assistance or healthcare benefits?"
High risk,"Does the AI calculate credit scores or insurance risk/pricing for individual consumers?"
High risk,"Is the system used to prioritise emergency‑response resources or medical triage for patients?"
High risk,"Does the AI support law‑enforcement or prosecutorial decisions (e.g., lie detection, evidence reliability, risk of re‑offending)?"
High risk,"Is the AI used by border or migration authorities to assess security, health, or migration risks, or to verify identity beyond document checks?"
High risk,"Does the AI assist judges, courts, or arbitration bodies in applying law or resolving disputes?"
High risk,"Is the system designed to tailor political messaging with the intent to influence the outcome of an election or referendum?"

Limited risk,"Does the AI interact autonomously with users (chat, voice, avatar) without an upfront disclosure that the counterpart is artificial?"
Limited risk,"Does the system create synthetic media (text, images, audio, video) that is not automatically watermarked or labelled as AI‑generated?"
Limited risk,"Does the AI detect emotions or categorise individuals biometrically without first informing them and obtaining any legally required consent?"
Limited risk,"Does the system produce realistic deep‑fake content without a persistent, visible notice indicating it is artificial?"
Limited risk,"Does the AI autonomously publish text on matters of public interest without human editorial oversight and without disclosing its artificial origin?"
Limited risk,"If transparency would normally be required, is the deployer claiming a special law‑enforcement exemption to withhold disclosure?"
